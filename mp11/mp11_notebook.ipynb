{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS440/ECE448 Spring 2023\n",
    "# MP11: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to do is to download this file: <a href=\"mp11.zip\">mp11.zip</a>.  If you want, you can also download <a href=\"mp11_extra.zip\">mp11_extra.zip</a>, the extra credit assignment.  `mp11.zip` has the following content:\n",
    "\n",
    "* `submitted.py`: Your homework. Edit, and then submit to <a href=\"https://www.gradescope.com/courses/486387\">Gradescope</a>.\n",
    "* `mp11_notebook.ipynb`: This is a <a href=\"https://anaconda.org/anaconda/jupyter\">Jupyter</a> notebook to help you debug.  You can completely ignore it if you want, although you might find that it gives you useful instructions.\n",
    "* `pong.py`: This is a program that plays Pong.  If called interactively, it will call the module `pong_display.py` to create a display, so that you can play.  If told to use a Q-learner, it will call your `submitted.py` to do Q-learning.\n",
    "* `grade.py`: Once your homework seems to be working, you can test it by typing `python grade.py`, which will run the tests in `tests/tests_visible.py`.\n",
    "* `tests/test_visible.py`: This file contains about half of the <a href=\"https://docs.python.org/3/library/unittest.html\">unit tests</a> that Gradescope will run in order to grade your homework.  If you can get a perfect score on these tests, then you should also get a perfect score on the additional hidden tests that Gradescope uses.\n",
    "* `requirements.txt`: This tells you which python packages you need to have installed, in order to run `grade.py`.  You can install all of those packages by typing `pip install -r requirements.txt` or `pip3 install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file (`mp11_notebook.ipynb`) will walk you through the whole MP, giving you instructions and debugging tips as you go.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. <a href=\"#section1\">Playing Pong</a>\n",
    "1. <a href=\"#section2\">Creating a Q-Learner Object</a>\n",
    "1. <a href=\"#section3\">Epsilon-First Exploration</a>\n",
    "1. <a href=\"#section4\">Q-Learning</a>\n",
    "1. <a href=\"#section5\">Saving and Loading Your Q and N Tables</a>\n",
    "1. <a href=\"#section6\">Exploitation</a>\n",
    "1. <a href=\"#section7\">Acting</a>\n",
    "1. <a href=\"#section8\">Training</a>\n",
    "1. <a href=\"#section9\">Extra Credit</a>\n",
    "1. <a href=\"#grade\">Grade Your Homework</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pong was the <a href=\"https://en.wikipedia.org/wiki/Pong\">first video game produced by Atari.</a>  It is a simple game, based on table tennis.  Here is a two-person version of the game: https://commons.wikimedia.org/wiki/File:Pong_Game_Test2.gif\n",
    "\n",
    "We will be playing a one-person version of the game:\n",
    "\n",
    "* When the ball hits the top, bottom, or left wall of the playing field, it bounces.\n",
    "* The right end of the playing field is open, except for the paddle.  If the ball hits the paddle, it bounces, and the player's score increments by one.  If the ball hits the open space, the game is over; the score resets to zero, and a new game begins.\n",
    "\n",
    "The game is pretty simple, but in order to get a better feeling for it, you may want to try playing it yourself.  Use the up arrow to move the paddle upward, and the down arrow to move the paddle downward.  See how high you can make your score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python pong.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you figure out how to use the arrow keys to control your paddle, we hope you will find that the game is not too hard for a human to play.  However, for a computer, it's difficult to know: where should the paddle be moved at each time step?  In order to see how difficult it is for a computer to play, let's ask the \"random\" player to play the game.\n",
    "\n",
    "**WARNING:** The following line will open a pygame window.  The pygame window will be hidden by this window -- in order to see it, you will need to minimize this window.  The pygame window will consume a lot of CPU time just waiting for the processor, so in order to kill it, you will need to come back to this window, click on the block below, then click the Jupyter \"stop\" button (the square button at the top of this window) in order to stop processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python pong.py --player random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Q-Learner Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you will do is to create a `q_learner` object that can store your learned Q table and your N table (table of exploration counts).  \n",
    "\n",
    "Like any other object-oriented language, python permits you to create new object classes in order to store data that will be needed from time to time.  If you are not already very, very familiar with python classes, you might want to study the python class tutorial: https://docs.python.org/3/tutorial/classes.html\n",
    "\n",
    "Like any other object in python, a `q_learner` object is created by calling its name as a function, e.g., `my_q_learner=submitted.q_learner()`.  Doing so calls the function `submitted.q_learner.__init__()`.  Let's look at the docstring to see what it should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module submitted:\n",
      "\n",
      "__init__(self, alpha, epsilon, gamma, nfirst, state_cardinality)\n",
      "    Create a new q_learner object.\n",
      "    Your q_learner object should store the provided values of alpha,\n",
      "    epsilon, gamma, and nfirst.\n",
      "    It should also create a Q table and an N table.\n",
      "    Q[...state..., ...action...] = expected utility of state/action pair.\n",
      "    N[...state..., ...action...] = # times state/action has been explored.\n",
      "    Both are initialized to all zeros.\n",
      "    Up to you: how will you encode the state and action in order to\n",
      "    define these two lookup tables?  The state will be a list of 5 integers,\n",
      "    such that 0 <= state[i] < state_cardinality[i] for 0 <= i < 5.\n",
      "    The action will be either -1, 0, or 1.\n",
      "    It is up to you to decide how to convert an input state and action\n",
      "    into indices that you can use to access your stored Q and N tables.\n",
      "    \n",
      "    @params:\n",
      "    alpha (scalar) - learning rate of the Q-learner\n",
      "    epsilon (scalar) - probability of taking a random action\n",
      "    gamma (scalar) - discount factor        \n",
      "    nfirst (scalar) - exploring each state/action pair nfirst times before exploiting\n",
      "    state_cardinality (list) - cardinality of each of the quantized state variables\n",
      "    \n",
      "    @return:\n",
      "    None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import submitted, importlib\n",
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your `__init__` function to meet the requirements specified in the docstring.  Once you have completed it, the following code should run without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<submitted.q_learner object at 0x000002422AB13BB0>\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "\n",
    "q_learner = submitted.q_learner(0.05,0.05,0.99,5,[10,10,2,2,10])\n",
    "\n",
    "print(q_learner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-First Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to manage the exploration/exploitation tradeoff, we will be using both \"epsilon-first\" and \"epsilon-greedy\" (https://en.wikipedia.org/wiki/Multi-armed_bandit#Semi-uniform_strategies).  \n",
    "\n",
    "The epsilon-first strategy explores every state/action pair at least `nfirst` times before it ever starts to exploit any strategy.  Your `q_learner` should have a table to keep track of how many times it has explored a state/action pair prior to the start of any exploitation.  The method for storing that table is up to you; in order to have some standardized API, therefore, you need to write a method called `report_exploration_counts` that returns a list of the three exploration counts for a given state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function report_exploration_counts in module submitted:\n",
      "\n",
      "report_exploration_counts(self, state)\n",
      "    Check to see how many times each action has been explored in this state.\n",
      "    @params:\n",
      "    state (list of 5 ints): ball_x, ball_y, ball_vx, ball_vy, paddle_y.\n",
      "      These are the (x,y) position of the ball, the (vx,vy) velocity of the ball,\n",
      "      and the y-position of the paddle, all quantized.\n",
      "      0 <= state[i] < state_cardinality[i], for all i in [0,4].\n",
      "    \n",
      "    @return:\n",
      "    explored_count (array of 3 ints): \n",
      "      number of times that each action has been explored from this state.\n",
      "      The mapping from actions to integers is up to you, but there must be three of them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.report_exploration_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `report_exploration_counts` so that it returns a list or array for any given state.  Test your code with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how many times state [0,0,0,0,0] has been explored so far:\n",
      "[0, 0, 0]\n",
      "This is how many times state [9,9,1,1,9] has been explored so far:\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "q_learner = submitted.q_learner(0.05,0.05,0.99,5,[10,10,2,2,10])\n",
    "print('This is how many times state [0,0,0,0,0] has been explored so far:')\n",
    "print(q_learner.report_exploration_counts([0,0,0,0,0]))\n",
    "print('This is how many times state [9,9,1,1,9] has been explored so far:')\n",
    "print(q_learner.report_exploration_counts([9,9,1,1,9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your learner first starts learning, it will call the function `choose_unexplored_action` to choose an unexplored action.  This function should choose a function uniformly at random from the set of unexplored actions in a given state, if there are any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function choose_unexplored_action in module submitted:\n",
      "\n",
      "choose_unexplored_action(self, state)\n",
      "    Choose an action that has been explored less than nfirst times.\n",
      "    If many actions are underexplored, you should choose uniformly\n",
      "    from among those actions; don't just choose the first one all\n",
      "    the time.\n",
      "    \n",
      "    @params:\n",
      "    state (list of 5 ints): ball_x, ball_y, ball_vx, ball_vy, paddle_y.\n",
      "       These are the (x,y) position of the ball, the (vx,vy) velocity of the ball,\n",
      "      and the y-position of the paddle, all quantized.\n",
      "      0 <= state[i] < state_cardinality[i], for all i in [0,4].\n",
      "    \n",
      "    @return:\n",
      "    action (scalar): either -1, or 0, or 1, or None\n",
      "      If all actions have been explored at least n_explore times, return None.\n",
      "      Otherwise, choose one uniformly at random from those w/count less than n_explore.\n",
      "      When you choose an action, you should increment its count in your counter table.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.choose_unexplored_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this has been written correctly, the following block should generate a random sequence of actions.  If the next block produces the same action 5 times in a row, that is the wrong result, and the result would be that your code does not pass the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next action: 0\n",
      "Next action: -1\n",
      "Next action: 1\n",
      "Next action: 0\n",
      "Next action: -1\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "q_learner = submitted.q_learner(0.05,0.05,0.99,5,[10,10,2,2,10])\n",
    "print('Next action:',q_learner.choose_unexplored_action([9,9,1,1,9]))\n",
    "print('Next action:',q_learner.choose_unexplored_action([9,9,1,1,9]))\n",
    "print('Next action:',q_learner.choose_unexplored_action([9,9,1,1,9]))\n",
    "print('Next action:',q_learner.choose_unexplored_action([9,9,1,1,9]))\n",
    "print('Next action:',q_learner.choose_unexplored_action([9,9,1,1,9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all three actions have been explored `nfirst` times, the function `choose_unexplored_action` should return `None`, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next action: 0\n",
      "Next action: 1\n",
      "Next action: -1\n",
      "Next action: None\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "q_learner = submitted.q_learner(0.05,0.05,0.99,1,[10,10,2,2,10])\n",
    "print('Next action:',q_learner.choose_unexplored_action([9,9,1,1,9]))\n",
    "print('Next action:',q_learner.choose_unexplored_action([9,9,1,1,9]))\n",
    "print('Next action:',q_learner.choose_unexplored_action([9,9,1,1,9]))\n",
    "print('Next action:',q_learner.choose_unexplored_action([9,9,1,1,9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reinforcement learning we are implementing is called Q-learning (https://en.wikipedia.org/wiki/Q-learning).  \n",
    "\n",
    "Q-learning keeps a table $Q[s,a]$ that specifies the expected utility of action $a$ in state $s$.  The organization of this table is up to you.  In order to have a standard API, the first thing you should implement is a function `report_q` with the following docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function report_q in module submitted:\n",
      "\n",
      "report_q(self, state)\n",
      "    Report the current Q values for the given state.\n",
      "    @params:\n",
      "    state (list of 5 ints): ball_x, ball_y, ball_vx, ball_vy, paddle_y.\n",
      "      These are the (x,y) position of the ball, the (vx,vy) velocity of the ball,\n",
      "      and the y-position of the paddle, all quantized.\n",
      "      0 <= state[i] < state_cardinality[i], for all i in [0,4].\n",
      "    \n",
      "    @return:\n",
      "    Q (array of 3 floats): \n",
      "      reward plus expected future utility of each of the three actions. \n",
      "      The mapping from actions to integers is up to you, but there must be three of them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.report_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your `q_learner` is first initialized, the value of $Q[state,action]$ should be zero for all state/action pairs, thus the `report_q` function should return lists of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[0,0,0,0,0] is now: [0.0, 0.0, 0.0]\n",
      "Q[9,9,1,1,9] is now: [0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "q_learner=submitted.q_learner(0.05,0.05,0.99,5,[10,10,2,2,10])\n",
    "print('Q[0,0,0,0,0] is now:',q_learner.report_q([0,0,0,0,0]))\n",
    "print('Q[9,9,1,1,9] is now:',q_learner.report_q([9,9,1,1,9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually many different Q-learning algorithms available, but when people refer to Q-learning with no modifier, they usually mean the time-difference (TD) algorithm.  For example, this is the algorithm that's described on the wikipedia page (https://en.wikipedia.org/wiki/Q-learning).  This is the algorithm you will implement for this MP.\n",
    "\n",
    "In supervised machine learning, the learner tries to imitate a reference label.  In reinforcement learning, there is no reference label.  Q-learning replaces the reference label with a \"local Q\" value, which is the utility that was obtained by performing action $a$ in state $s$ one time.  It is usually calculated like this:\n",
    "\n",
    "$$Q_{local}(s_t,a_t) = r_t + \\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})$$\n",
    "\n",
    "where $r_t$ is the reward that was achieved by performing action $a_t$ in state $s_t$, $s_{t+1}$ is the state into which the game transitioned, and $a_{t+1}$ is one of the actions that could be performed in that state.  $Q_{local}$ is computed by your `q_local` function, which has this docstring:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function q_local in module submitted:\n",
      "\n",
      "q_local(self, reward, newstate)\n",
      "    The update to Q estimated from a single step of game play:\n",
      "    reward plus gamma times the max of Q[newstate, ...].\n",
      "    \n",
      "    @param:\n",
      "    reward (scalar float): the reward achieved from the current step of game play.\n",
      "    newstate (list of 5 ints): ball_x, ball_y, ball_vx, ball_vy, paddle_y.\n",
      "      These are the (x,y) position of the ball, the (vx,vy) velocity of the ball,\n",
      "      and the y-position of the paddle, all quantized.\n",
      "      0 <= state[i] < state_cardinality[i], for all i in [0,4].\n",
      "    \n",
      "    @return:\n",
      "    Q_local (scalar float): the local value of Q\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.q_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, `q_local` should just return the given reward, because initially, all Q values are 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_local(6.25,[9,9,1,1,9]) is currently: 6.25\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "q_learner = submitted.q_learner(0.05,0.05,0.99,5,[10,10,2,10,10])\n",
    "print('Q_local(6.25,[9,9,1,1,9]) is currently:',q_learner.q_local(6.25,[9,9,1,1,9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use `q_learner.q_local` as the target for `q_learner.learn`.  The basic algorithm is\n",
    "\n",
    "$$Q(s,a) = Q(s,a) + \\alpha (Q_{local}(s,a)-Q(s,a))$$\n",
    "\n",
    "Here is the docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function learn in module submitted:\n",
      "\n",
      "learn(self, state, action, reward, newstate)\n",
      "    Update the internal Q-table on the basis of an observed\n",
      "    state, action, reward, newstate sequence.\n",
      "    \n",
      "    @params:\n",
      "    state: a list of 5 numbers: ball_x, ball_y, ball_vx, ball_vy, paddle_y.\n",
      "      These are the (x,y) position of the ball, the (vx,vy) velocity of the ball,\n",
      "      and the y-position of the paddle.\n",
      "    action: an integer, one of -1, 0, or +1\n",
      "    reward: a reward; positive for hitting the ball, negative for losing a game\n",
      "    newstate: a list of 5 numbers, in the same format as state\n",
      "    \n",
      "    @return:\n",
      "    None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block checks a sequence of Q updates:\n",
    "\n",
    "1. First, $Q([9,9,1,1,9],-1)$ is updated.  Since all Q values start at zero, it will be updated to just have a value equal to $\\alpha$ (0.05) times the given reward (6.25) for a total value of 0.3125.\n",
    "1. When we print out $Q([9,9,1,1,9],:)$, we see that one of the elements has been updated.\n",
    "1. Next, update $Q([9,9,1,1,8],1)$ with a given reward, and with $[9,9,1,1,9]$ as the given next state.  Since $Q([9,9,1,1,9],-1)$ is larger than zero, the next-state Q-value should be multiplied by $\\gamma$ (0.99) and added to the reward (3.1), then multiplied by $\\alpha$, giving a total value of 0.17046875.\n",
    "1. The resulting Q-value is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[9,9,1,1,9] is now [0.3125, 0.0, 0.0]\n",
      "Q[9,9,1,1,8] is now [0.0, 0.0, 0.17046875000000003]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "q_learner = submitted.q_learner(0.05,0.05,0.99,5,[10,10,2,2,10])\n",
    "q_learner.learn([9,9,1,1,9],-1,6.25,[0,0,0,0,0])\n",
    "print('Q[9,9,1,1,9] is now',q_learner.report_q([9,9,1,1,9]))\n",
    "q_learner.learn([9,9,1,1,8],1,3.1,[9,9,1,1,9])\n",
    "print('Q[9,9,1,1,8] is now',q_learner.report_q([9,9,1,1,8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading your Q and N Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've spent a long time training your `q_learner`, you will want to save your Q and N tables so that you can reload them later.  The format of Q and N is up to you, therefore it's also up to you to write the `save` and `load` functions.  Here are the docstrings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function save in module submitted:\n",
      "\n",
      "save(self, filename)\n",
      "    Save your Q and N tables to a file.\n",
      "    This can save in any format you like, as long as your \"load\" \n",
      "    function uses the same file format.  We recommend numpy.savez,\n",
      "    but you can use something else if you prefer.\n",
      "    \n",
      "    @params:\n",
      "    filename (str) - filename to which it should be saved\n",
      "    @return:\n",
      "    None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load in module submitted:\n",
      "\n",
      "load(self, filename)\n",
      "    Load the Q and N tables from a file.\n",
      "    This should load from whatever file format your save function\n",
      "    used.  We recommend numpy.load, but you can use something\n",
      "    else if you prefer.\n",
      "    \n",
      "    @params:\n",
      "    filename (str) - filename from which it should be loaded\n",
      "    @return:\n",
      "    None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions can be tested by doing one step of training one `q_learner`, then saving its results, then loading them into another `q_learner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next action: 0\n",
      "N1[9,9,1,1,8] is now [0, 1, 0]\n",
      "Q1[9,9,1,1,8] is now [0.3125, 0.0, 0.0]\n",
      "N2[9,9,1,1,8] starts out as [0, 0, 0]\n",
      "Q2[9,9,1,1,8] starts out as [0.0, 0.0, 0.0]\n",
      "N2[9,9,1,1,8] is now [0, 1, 0]\n",
      "Q2[9,9,1,1,8] is now [0.3125, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "q_learner1 = submitted.q_learner(0.05,0.05,0.99,5,[10,10,2,2,10])\n",
    "print('Next action:',q_learner1.choose_unexplored_action([9,9,1,1,9]))\n",
    "q_learner1.learn([9,9,1,1,9],-1,6.25,[0,0,0,0,0])\n",
    "print('N1[9,9,1,1,8] is now',q_learner1.report_exploration_counts([9,9,1,1,9]))\n",
    "print('Q1[9,9,1,1,8] is now',q_learner1.report_q([9,9,1,1,9]))\n",
    "q_learner1.save('test.npz')\n",
    "\n",
    "q_learner2 = submitted.q_learner(0.05,0.05,0.99,5,[10,10,2,2,10])\n",
    "print('N2[9,9,1,1,8] starts out as',q_learner2.report_exploration_counts([9,9,1,1,9]))\n",
    "print('Q2[9,9,1,1,8] starts out as',q_learner2.report_q([9,9,1,1,9]))\n",
    "q_learner2.load('test.npz')\n",
    "print('N2[9,9,1,1,8] is now',q_learner2.report_exploration_counts([9,9,1,1,9]))\n",
    "print('Q2[9,9,1,1,8] is now',q_learner2.report_q([9,9,1,1,9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reinforcement learner always has to trade off between exploration (choosing an action at random) versus exploitation (choosing the action with the maximum expected utility).  Before we worry about that tradeoff, though, let's first make sure that exploitation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function exploit in module submitted:\n",
      "\n",
      "exploit(self, state)\n",
      "    Return the action that has the highest Q-value for the current state, and its Q-value.\n",
      "    @params:\n",
      "    state (list of 5 ints): ball_x, ball_y, ball_vx, ball_vy, paddle_y.\n",
      "      These are the (x,y) position of the ball, the (vx,vy) velocity of the ball,\n",
      "      and the y-position of the paddle, all quantized.\n",
      "      0 <= state[i] < state_cardinality[i], for all i in [0,4].\n",
      "    \n",
      "    @return:\n",
      "    action (scalar int): either -1, or 0, or 1.\n",
      "      The action that has the highest Q-value.  Ties can be broken any way you want.\n",
      "    Q (scalar float): \n",
      "      The Q-value of the selected action\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.exploit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1[9,9,1,1,9] is now [0.3125, 0.0, 0.0]\n",
      "The best action and Q from state [9,9,1,1,9] are (-1, 0.3125)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "q_learner1 = submitted.q_learner(0.05,0.05,0.99,5,[10,10,2,2,10])\n",
    "q_learner1.learn([9,9,1,1,9],-1,6.25,[0,0,0,0,0])\n",
    "print('Q1[9,9,1,1,9] is now',q_learner1.report_q([9,9,1,1,9]))\n",
    "print('The best action and Q from state [9,9,1,1,9] are',q_learner1.exploit([9,9,1,1,9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your learner decides which action to perform, it should trade off exploration vs. exploitation using both the epsilon-first and the epsilon-greedy strategies:\n",
    "1. If there is any action that has been explored fewer than `nfirst` times, then choose one of those actions at random.  Otherwise...\n",
    "1. With probability `epsilon`, choose an action at random.  Otherwise...\n",
    "1. Exploit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function act in module submitted:\n",
      "\n",
      "act(self, state)\n",
      "    Decide what action to take in the current state.\n",
      "    If any action has been taken less than nfirst times, then choose one of those\n",
      "    actions, uniformly at random.\n",
      "    Otherwise, with probability epsilon, choose an action uniformly at random.\n",
      "    Otherwise, choose the action with the best Q(state,action).\n",
      "    \n",
      "    @params: \n",
      "    state: a list of 5 integers: ball_x, ball_y, ball_vx, ball_vy, paddle_y.\n",
      "      These are the (x,y) position of the ball, the (vx,vy) velocity of the ball,\n",
      "      and the y-position of the paddle, all quantized.\n",
      "      0 <= state[i] < state_cardinality[i], for all i in [0,4].\n",
      "    \n",
      "    @return:\n",
      "    -1 if the paddle should move upward\n",
      "    0 if the paddle should be stationary\n",
      "    1 if the paddle should move downward\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.q_learner.act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test all three types of action (epsilon-first exploration, epsilon-greedy exploration, and exploitation), let's create a learner with `nfirst=1` and `epsilon=0.25`, and set it so that the best action from state `[9,9,1,1,9]` is `-1`.  With these settings, a sequence of calls to `q_learner.act` should produce the following sequence of actions:\n",
    "\n",
    "1. The first three actions should include each possible action once.\n",
    "1. After the first three actions, 3/4 of the remaining actions should be `-1`.  The remaining 1/4 should be randomly chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An epsilon-first action: 1\n",
      "An epsilon-first action: 0\n",
      "An epsilon-first action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: 0\n",
      "An epsilon-greedy explore/exploit action: -1\n",
      "An epsilon-greedy explore/exploit action: -1\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "q_learner=submitted.q_learner(0.05,0.25,0.99,1,[10,10,2,2,10])\n",
    "q_learner.learn([9,9,1,1,9],-1,6.25,[0,0,0,0,0])\n",
    "print('An epsilon-first action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-first action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-first action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))\n",
    "print('An epsilon-greedy explore/exploit action:',q_learner.act([9,9,1,1,9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of your components work, you can try training your algorithm.  Do this by giving your `q_learner` as a player to a new `pong.PongGame` object.  Set `visibility=False` so that the `PongGame` doesn't create a new window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module pong:\n",
      "\n",
      "__init__(self, ball_speed=4, paddle_speed=8, learner=None, visible=True, state_quantization=[10, 10, 2, 2, 10])\n",
      "    Create a new pong game, with a specified player.\n",
      "    \n",
      "    @params:\n",
      "    ball_speed (scalar int) - average ball speed in pixels/frame\n",
      "    paddle_speed (scalar int) - paddle moves 0, +paddle_speed, or -paddle_speed\n",
      "    learner - can be None if the player is human.  If not None, should be an \n",
      "      object of type random_learner, submitted.q_learner, or submitted.deep_q.\n",
      "    visible (bool) - should this game have an attached pygame window?\n",
      "    state_quantization (list) - if not None, state variables are quantized\n",
      "      into integers of these cardinalities before being passed to the learner.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pong, importlib, submitted\n",
    "importlib.reload(pong)\n",
    "help(pong.PongGame.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we should set `visibility=False` so that the `PongGame` doesn't create a new window.  We should also make sure that the PongGame uses the same state quantization as the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "<pong.PongGame object at 0x00000242330DE910>\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pong)\n",
    "importlib.reload(submitted)\n",
    "state_quantization = [10,10,2,2,10]\n",
    "q_learner=submitted.q_learner(0.05,0.05,0.99,5,state_quantization)\n",
    "\n",
    "pong_game = pong.PongGame(learner=q_learner, visible=True, state_quantization=state_quantization)\n",
    "print(pong_game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our learner, we want it to play the game many times.  To do that we use the PongGame.run function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method run in module pong:\n",
      "\n",
      "run(m_rewards=inf, m_games=inf, m_frames=inf, states=[]) method of pong.PongGame instance\n",
      "    Run the game.\n",
      "    @param\n",
      "    m_frames (scalar int): maximum number of frames to be played\n",
      "    m_rewards (scalar int): maximum number of rewards earned (+ or -)\n",
      "    m_games (scalar int): maximum number of games\n",
      "    states (list): list of states whose Q-values should be returned\n",
      "       each state is a list of 5 ints: ball_x, ball_y, ball_vx, ball_vy, paddle_y.\n",
      "       These are the (x,y) position of the ball, the (vx,vy) velocity of the ball,\n",
      "       and the y-position of the paddle, all quantized.\n",
      "       0 <= state[i] < state_cardinality[i], for all i in [0,4].\n",
      "    \n",
      "      \n",
      "    @return\n",
      "    scores (list): list of scores of all completed games\n",
      "    \n",
      "    The following will be returned only if the player is q_learning or deep_q.\n",
      "    New elements will be added to these lists once/frame if m_frames is specified,\n",
      "    else once/reward if m_rewards is specified, else once/game:\n",
      "      q_achieved (list): list of the q-values of the moves that were taken\n",
      "      q_states (list): list of the q-values of requested states\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pong_game.run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure our learner is learning, let's tell `pong_game.run` to output all 3 Q-values of all of the 4000 states in every time step.\n",
    "\n",
    "To make sure that's not an outrageous amount of data, let's tell it to only output the Q values once/reward, and ask it to only collect 5000 rewards:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 games, 2 rewards, 306 frames, score 1, max score 1\n",
      "Completed 1 games, 5 rewards, 756 frames, score 2, max score 2\n",
      "Completed 2 games, 8 rewards, 1306 frames, score 2, max score 2\n"
     ]
    }
   ],
   "source": [
    "states = [[x,y,vx,vy,py] for x in range(10) for y in range(10) for vx in range(2) for vy in range(2) for py in range(10) ]\n",
    "\n",
    "scores, q_achieved, q_states = pong_game.run(m_rewards=500, states=states)\n",
    "\n",
    "print('The number of games played was',len(scores))\n",
    "print('The number of rewards was',len(q_states))\n",
    "print('The size of each returned Q-matrix was',q_states[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned value of `q_states` is a list of 4000x3 numpy arrays (20 states, 3 actions).  The list contains `m_rewards` of these. We want to convert it into something that matplotlib can plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q = np.array([np.reshape(q,-1) for q in q_states])\n",
    "print('Q is now of shape',Q.shape)\n",
    "print('the max absolute value of Q is ',np.amax(abs(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(14,6),layout='tight')\n",
    "ax = [ fig.add_subplot(2,1,x) for x in range(1,3) ]\n",
    "ax[0].plot(np.arange(0,len(q_states)),Q)\n",
    "ax[0].set_title('Q values of all states')\n",
    "ax[1].plot(np.arange(0,len(q_states)),q_achieved)\n",
    "ax[1].set_title('Q values of state achieved at each time')\n",
    "ax[1].set_ylabel('Reward number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's try running it for a much longer period -- say, 5000 complete games.  We won't ask it to print out any states this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "scores, q_achieved, q_states = pong_game.run(m_games=5000, states=[])\n",
    "\n",
    "print('The number of games played was',len(scores))\n",
    "print('The number of video frames was',len(q_states))\n",
    "print('The size of each returned Q-matrix was',q_states[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the score, to see if it improved over time.  We will also plot the local average, averaged over 10 consecutive games, to see if that has improved.  Notice that we can use `np.convolve` to compute the local average.\n",
    "\n",
    "These numbers are really noisy, with a really large maximum.  We will plot `np.log10(1+x)`, rather than x, so that we can better see the average numbers, and ignore the very large noisy spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(14,9),layout='tight')\n",
    "ax = [ fig.add_subplot(3,1,x) for x in range(1,4) ]\n",
    "ax[0].plot(np.arange(0,len(scores)),np.log10(1+np.array(scores)))\n",
    "ax[0].plot([0,5000],np.log10([7,7]),'k--')\n",
    "ax[0].set_title('Game scores')\n",
    "ax[1].plot(np.arange(4991),np.log10(1+np.convolve(np.ones(10)/10,scores,mode='valid')))\n",
    "ax[1].plot([0,4991],np.log10([7,7]),'k--')\n",
    "ax[1].set_title('Game scores, average 10 consecutive games')\n",
    "ax[2].plot(np.arange(0,len(q_achieved)),q_achieved)\n",
    "ax[2].set_title('Q values of state achieved at each time')\n",
    "ax[2].set_ylabel('Game number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, it has learned!  If you are getting a ten-game average score of better than 6, then you are ready to submit your model for grading.   In order to do that, you need to save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "q_learner.save('trained_model.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extra credit, download the file <a href=\"mp11_extra.zip\">mp11_extra.zip</a>.  The only important file in this package is:\n",
    "* `tests/test_extra.py`: this contains extra tests that will evaluate your pre-trained `deep_q` learner, which should be in a file called `trained_model.pkl`.  For full credit, your model should achieve an average score of greater than 20, averaged over 10 consecutive games. \n",
    "\n",
    "With a quantized lookup table, it's probably not possible to achieve an average score of 20.  With a deep-Q learner, however, it is eminently possible.  In order to do the extra credit, therefore, you should just fill in the part of `submitted.py` that implements the `deep_q` learner, using pytorch to define a model structure, train it, save it, load it, and act on it.  This learner only needs to have five methods: `__init__`, `act`, `learn`, `save`, and `load`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.deep_q.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "help(submitted.deep_q.act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "help(submitted.deep_q.learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "help(submitted.deep_q.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "help(submitted.deep_q.load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grade'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade your homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've reached this point, and all of the above sections work, then you're ready to try grading your homework!  Before you submit it to Gradescope, try grading it on your own machine.  This will run some visible test cases (which you can read in `tests/test_visible.py`), and compare the results to the solutions (which you can read in `solution.json`).\n",
    "\n",
    "The exclamation point (!) tells python to run the following as a shell command.  Obviously you don't need to run the code this way -- this usage is here just to remind you that you can also, if you wish, run this command in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!python grade.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should try uploading your code to <a href=\"https://www.gradescope.com/courses/486387\">Gradescope</a>.  \n",
    "\n",
    "**Warning:** For this MP you need to update two files, not just one. \n",
    "* `trained_model.npz` should contain your trained model\n",
    "* `submitted.py` should contain your code that loads and runs your trained model.\n",
    "\n",
    "**Warning:** The autograder calculates the average score over ten random games.  If you are getting an average score above 10 almost all the time on your own computer, and if the autograder says you had a score below 10, try resubmitting to see if the next round of random games is better.\n",
    "\n",
    "**Extra Credit:** Your extra credit should also be uploaded as two files,\n",
    "* `trained_model.pkl` should contain your trained model\n",
    "* `submitted.py` should contain your code that loads and runs your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
